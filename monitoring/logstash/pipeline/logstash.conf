input {
  # TCP input for Logstash to receive JSON logs
  tcp {
    port => 5000
    codec => json_lines
  }
  
  # File input for reading log files
  file {
    path => "/logs/**/*.log"
    start_position => "beginning"
    codec => json
    tags => ["file"]
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }
  
  # Extract service name
  if [service_name] {
    mutate {
      add_field => { "[@metadata][index_prefix]" => "%{service_name}" }
    }
  } else {
    mutate {
      add_field => { "[@metadata][index_prefix]" => "farm-monitoring" }
    }
  }
  
  # Parse timestamp
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }
  
  # Add geolocation if IP is present
  if [client_ip] {
    geoip {
      source => "client_ip"
    }
  }
  
  # Categorize log levels
  if [level] {
    mutate {
      lowercase => [ "level" ]
    }
  }
  
  # Extract trace context
  if [traceId] {
    mutate {
      add_field => {
        "trace_context" => "%{traceId}/%{spanId}"
      }
    }
  }
  
  # Tag critical errors
  if [level] == "error" or [level] == "fatal" {
    mutate {
      add_tag => [ "critical" ]
    }
  }
  
  # Remove unnecessary fields
  mutate {
    remove_field => [ "host", "path", "type" ]
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
    user => "${ELASTICSEARCH_USER:elastic}"
    password => "${ELASTICSEARCH_PASSWORD:changeme}"
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
    template_overwrite => true
  }
  
  # Debug output to stdout (disable in production)
  stdout {
    codec => rubydebug
  }
}
